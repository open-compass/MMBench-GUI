EVAL_WORK_DIR=./evaluation_results

# If you put images in another path rather than `LMUDataRoot()/MMBench-GUI/offline_images`, you can set and use this variable.
# Check our example (models/local_uitars.py) for details.
#   IMAGE_ROOT_DIR=/mnt/petrelfs/wangxuehui/project/computer_use/MMBench-GUI/release/offline_images

# Please UNCOMMENT the line if you want to provide your user prompt for level 2 without writing your own build_prompt function.
# In fact, we reconmmond that you implement custom_build_prompt in your model file to fully control the prompt.
#   L2_USER_PROMPT="your user prompt with {instruction}"

# When calculating accuracy for level1, you can choose to uncomment this line to enable the weighted accuracy.
# In this implementation, we consider the number of options when making the score. This means that:
# 1. if the agent answer one question correctly and this question has 5 options, the score is 4/5=0.8 rather then 1.0
# 2. if the agent answer one question correctly and this question has 4 options, the score is 3/4=0.75 rather then 1.0 
#   L1_USE_WEIGHTED = 1
